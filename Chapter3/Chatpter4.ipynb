{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9ee79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cbfc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.36.0 to work with mlopsdev\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "import pandas as pd\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9818a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('./Data/titanic.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9fe5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].apply(lambda x: x.fillna(x.median()))\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbc6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Loc']= df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491e1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Cabin', 'Ticket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d96bd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch     Fare Embarked Loc  \n",
       "0      0   7.2500        S   X  \n",
       "1      0  71.2833        C   C  \n",
       "2      0   7.9250        S   X  \n",
       "3      0  53.1000        S   C  \n",
       "4      0   8.0500        S   X  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8678209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Fare           0\n",
       "Embarked       2\n",
       "Loc            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb61bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'GroupSize'] = 1 + df['SibSp'] + df['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87006ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embarked'] = df['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "376cf6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Loc</th>\n",
       "      <th>GroupSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age     Fare Embarked Loc  GroupSize\n",
       "0         0       3    male  22.0   7.2500        S   X          2\n",
       "1         1       1  female  38.0  71.2833        C   C          2\n",
       "2         1       3  female  26.0   7.9250        S   X          1\n",
       "3         1       1  female  35.0  53.1000        S   C          2\n",
       "4         0       3    male  35.0   8.0500        S   X          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL = 'Survived'\n",
    "columns_to_keep = ['Pclass', 'Sex','Age', 'Fare', 'Embared', 'Deck', 'GroupSize']\n",
    "columns_to_drop = ['Name','SibSp', 'Parch', 'Survived']\n",
    "df_train = df\n",
    "df = df_train.drop(['Name','SibSp', 'Parch', 'PassengerId'], axis=1)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "076d3b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/devbox/code/Users/babal/AMLBook2021/AMLBook2022/Chapter3/train_remote\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"train_remote\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ef4aa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Loc</th>\n",
       "      <th>GroupSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age     Fare Embarked Loc  GroupSize\n",
       "0         0       3    male  22.0   7.2500        S   X          2\n",
       "1         1       1  female  38.0  71.2833        C   C          2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('./train_remote/titanic.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f376d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./train_remote/titanic.csv\n",
      "Uploaded ./train_remote/titanic.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Dataset registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "#use default datastore retrieved from the workspace through the AML SDK\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "default_ds.upload_files(files=['./train_remote/titanic.csv'], # Upload the diabetes csv files in /data\n",
    "                        target_path= 'Titanic-data', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "#Create a tabular dataset from the path on the datastore \n",
    "dataset = Dataset.Tabular.from_delimited_files(default_ds.path('Titanic-data/titanic.csv'))\n",
    "\n",
    "# Register the dataset\n",
    "try:\n",
    "    tab_data_set = dataset.register(workspace=ws, \n",
    "                                name= 'Titanic-tabular-dataset',\n",
    "                                description='Tintanic data',\n",
    "                                tags = {'format':'csv'},\n",
    "                                create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4728d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Loc</th>\n",
       "      <th>GroupSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>888</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>21.5</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>X</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>889</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>890</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Column1  Survived  Pclass     Sex   Age     Fare Embarked Loc  GroupSize\n",
       "0          0         0       3    male  22.0   7.2500        S   X          2\n",
       "1          1         1       1  female  38.0  71.2833        C   C          2\n",
       "2          2         1       3  female  26.0   7.9250        S   X          1\n",
       "3          3         1       1  female  35.0  53.1000        S   C          2\n",
       "4          4         0       3    male  35.0   8.0500        S   X          1\n",
       "..       ...       ...     ...     ...   ...      ...      ...  ..        ...\n",
       "886      886         0       2    male  27.0  13.0000        S   X          1\n",
       "887      887         1       1  female  19.0  30.0000        S   B          1\n",
       "888      888         0       3  female  21.5  23.4500        S   X          4\n",
       "889      889         1       1    male  26.0  30.0000        C   C          1\n",
       "890      890         0       3    male  32.0   7.7500        Q   X          1\n",
       "\n",
       "[891 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = 'c46a9435-c957-4e6c-a0f4-b9a597984773'\n",
    "resource_group = 'mlops'\n",
    "workspace_name = 'mlopsdev'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='Titanic-tabular-dataset')\n",
    "dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68f84929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, 'titanic_remote_compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbddec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/devbox/code/Users/babal/AMLBook2021/AMLBook2022/Chapter3/train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"train\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ec3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/devbox/code/Users/babal/AMLBook2021/AMLBook2022/Chapter3/train/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/training.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Run, Dataset, Workspace, Experiment\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "\n",
    "# Calculate model performance metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "def getRuntimeArgs():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def buildpreprocessorpipeline(X_raw):\n",
    "    categorical_features = X_raw.select_dtypes(include=['object']).columns\n",
    "    numeric_features = X_raw.select_dtypes(include=['float','int64']).columns\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=\"missing\")),\n",
    "                                              ('onehotencoder', OneHotEncoder(categories='auto', sparse=False, handle_unknown='ignore'))])\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numeric', numeric_transformer, numeric_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)\n",
    "        ], remainder=\"drop\")\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def model_train(LABEL, df, run):  \n",
    "    y_raw = df[LABEL]\n",
    "    X_raw = df.drop([LABEL], axis=1)\n",
    "    \n",
    "     # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.3, random_state=0)\n",
    "    \n",
    "    lg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "    preprocessor = buildpreprocessorpipeline(X_train)\n",
    "    \n",
    "    #estimator instance\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lg)])\n",
    "\n",
    "    model = clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    run.log('AUC', np.float(auc))\n",
    "\n",
    "    \n",
    "    # calculate test accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "    run.log('Accuracy', np.float(acc))\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    run.log_image(name = \"ROC\", plot = fig)\n",
    "    plt.show()\n",
    "\n",
    "    # plot confusion matrix\n",
    "    # Generate confusion matrix\n",
    "    cmatrix = confusion_matrix(y_test, y_hat)\n",
    "    cmatrix_json = {\n",
    "        \"schema_type\": \"confusion_matrix\",\n",
    "           \"schema_version\": \"v1\",\n",
    "           \"data\": {\n",
    "               \"class_labels\": [\"0\", \"1\"],\n",
    "               \"matrix\": [\n",
    "                   [int(x) for x in cmatrix[0]],\n",
    "                   [int(x) for x in cmatrix[1]]\n",
    "               ]\n",
    "           }\n",
    "    }\n",
    "    \n",
    "    run.log_confusion_matrix('ConfusionMatrix_Test', cmatrix_json)\n",
    "\n",
    "    return model, auc, acc\n",
    "    # Save the trained model\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # Create an Azure ML experiment in your workspace\n",
    "    args = getRuntimeArgs()\n",
    "    \n",
    "    run = Run.get_context()\n",
    "\n",
    "    dataset_dir = './dataset/'\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    ws = run.experiment.workspace\n",
    "    print(ws)\n",
    "    \n",
    "\n",
    "    print(\"Loading Data...\")\n",
    "    dataset = Dataset.get_by_id(ws, id=args.input_data)\n",
    "    # Load a TabularDataset & save into pandas DataFrame\n",
    "    df = dataset.to_pandas_dataframe()\n",
    "    \n",
    "    print(df.head(5))\n",
    " \n",
    "    model, auc, acc = model_train('Survived', df, run)\n",
    "    \n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    \n",
    "    model_file = os.path.join('outputs', 'titanic_model.pkl')\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "    \n",
    "    run.upload_file(name='titanic_model.pkl', path_or_stream=model_file)\n",
    "    \n",
    "    # Register the model\n",
    "    print('Registering model...')\n",
    "    run.register_model(model_path='titanic_model.pkl', model_name= 'titanic-model',\n",
    "                   tags={'Model Type':'Logistic Regresssion'},\n",
    "                   properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
    "\n",
    "    run.complete()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24941269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/devbox/code/Users/babal/AMLBook2021/AMLBook2022/Chapter3/train/experiment_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/experiment_env.yml\n",
    "name: experiment_env\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3923fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment-env defined.\n",
      "name: experiment_env\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "- python=3.6.2\n",
      "- scikit-learn\n",
      "- ipykernel\n",
      "- matplotlib\n",
      "- pandas\n",
      "- pip\n",
      "- pip:\n",
      "  - azureml-defaults\n",
      "  - pyarrow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification('experiment-env', script_folder + \"/experiment_env.yml\")\n",
    "\n",
    "# Let Azure ML manage dependencies\n",
    "experiment_env.python.user_managed_dependencies = False \n",
    "\n",
    "# Print the environment details\n",
    "print(experiment_env.name, 'defined.')\n",
    "print(experiment_env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3053f0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d597228a82bd44749635313b1e81a3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/titanic_remote_compute_1643030740_5938d4b4?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/mlopsdev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\", \"run_id\": \"titanic_remote_compute_1643030740_5938d4b4\", \"run_properties\": {\"run_id\": \"titanic_remote_compute_1643030740_5938d4b4\", \"created_utc\": \"2022-01-24T13:25:40.53724Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"b14da3da-8307-42dd-b852-f895db1d5db0\", \"azureml.git.repository_uri\": \"https://github.com/balakreshnan/AMLBook2022.git\", \"mlflow.source.git.repoURL\": \"https://github.com/balakreshnan/AMLBook2022.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"fb0eb505a9199147bba52cf3ee1571aad910ebd2\", \"mlflow.source.git.commit\": \"fb0eb505a9199147bba52cf3ee1571aad910ebd2\", \"azureml.git.dirty\": \"False\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2022-01-24T13:25:56.208479Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/azureml-logs/60_control_log.txt?sv=2019-07-07&sr=b&sig=GO7qDHSI%2FiX1sGsidwBzIdr4xEWDuMwLSWVEsZ69tmQ%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A59Z&se=2022-01-24T21%3A25%3A59Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=eDMmvCLM4J65%2BVdaBCIm8i1thuJG5G5FAo3J3dE%2Bgnw%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A59Z&se=2022-01-24T21%3A25%3A59Z&sp=r\", \"logs/azureml/14301_azureml.log\": \"https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/14301_azureml.log?sv=2019-07-07&sr=b&sig=l59MURGsFgD5%2BJKcWtYCmscK7ERxLNT%2F2lIaiLqBWpg%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r\", \"logs/azureml/dataprep/backgroundProcess.log\": \"https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=%2BhLc11GyLmXD87P8k8RH8b1QJFO%2BELv9bl%2Bxvp0YBTw%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r\", \"logs/azureml/dataprep/backgroundProcess_Telemetry.log\": \"https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=vaGUKaGb8rjPaGO%2FF5i%2Fvy%2FgpgnFZ9reS8ksC3y%2BQ2c%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/dataprep/backgroundProcess.log\", \"logs/azureml/dataprep/backgroundProcess_Telemetry.log\"], [\"azureml-logs/60_control_log.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/14301_azureml.log\"]], \"run_duration\": \"0:00:15\", \"run_number\": \"5\", \"run_queued_details\": {\"status\": \"Completed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"AUC\", \"run_id\": \"titanic_remote_compute_1643030740_5938d4b4\", \"categories\": [0], \"series\": [{\"data\": [0.8492261904761904]}]}, {\"name\": \"Accuracy\", \"run_id\": \"titanic_remote_compute_1643030740_5938d4b4\", \"categories\": [0], \"series\": [{\"data\": [0.8097014925373134]}]}, {\"name\": \"ROC\", \"run_id\": \"titanic_remote_compute_1643030740_5938d4b4\", \"categories\": [0], \"series\": [{\"data\": [\"aml://artifactId/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/ROC_1643030746.png\"]}]}, {\"name\": \"ConfusionMatrix_Test\", \"run_id\": \"titanic_remote_compute_1643030740_5938d4b4\", \"categories\": [0], \"series\": [{\"data\": [{\"schema_type\": \"confusion_matrix\", \"schema_version\": \"v1\", \"data\": {\"class_labels\": [\"0\", \"1\"], \"matrix\": [[146, 22], [29, 71]]}}]}]}], \"run_logs\": \"2022-01-24 13:25:42,579|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'EnableMLflowTracking': True, 'snapshotProject': True}, track_folders: None, deny_list: None, directories_to_watch: ['logs', 'logs/azureml']\\n2022-01-24 13:25:42,585|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: none\\n2022-01-24 13:25:42,585|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Failed to import pyspark with error: No module named 'pyspark'\\n2022-01-24 13:25:42,585|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2022-01-24 13:25:42,976|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7f8a07030158> for run source azureml.scriptrun\\n2022-01-24 13:25:42,977|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2022-01-24 13:25:42,977|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2022-01-24 13:25:42,981|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2022-01-24 13:25:42,988|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2022-01-24 13:25:42,988|azureml.core.authentication|DEBUG|Time to expire 1814397.011647 seconds\\n2022-01-24 13:25:42,988|azureml._restclient.service_context|DEBUG|Created a static thread pool for ServiceContext class\\n2022-01-24 13:25:42,988|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:42,988|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:42,989|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:42,989|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:42,989|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:42,989|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:42,989|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:43,087|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2022-01-24 13:25:43,087|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2022-01-24 13:25:43,133|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:43,134|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'b14da3da-8307-42dd-b852-f895db1d5db0', 'azureml.git.repository_uri': 'https://github.com/balakreshnan/AMLBook2022.git', 'mlflow.source.git.repoURL': 'https://github.com/balakreshnan/AMLBook2022.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2', 'mlflow.source.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2', 'azureml.git.dirty': 'False'}\\n2022-01-24 13:25:43,134|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2022-01-24 13:25:43,134|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2022-01-24 13:25:43,134|azureml.WorkerPool|DEBUG|[START]\\n2022-01-24 13:25:43,134|azureml.SendRunKillSignal|DEBUG|[START]\\n2022-01-24 13:25:43,134|azureml.RunStatusContext|DEBUG|[START]\\n2022-01-24 13:25:43,134|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunContextManager.RunStatusContext|DEBUG|[START]\\n2022-01-24 13:25:43,135|azureml.MetricsClient|DEBUG|[START]\\n2022-01-24 13:25:43,135|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|[START]\\n2022-01-24 13:25:43,135|azureml.ContentUploader|DEBUG|[START]\\n2022-01-24 13:25:43,135|azureml._history.utils.context_managers|DEBUG|starting file watcher\\n2022-01-24 13:25:43,136|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Start]\\n2022-01-24 13:25:43,136|azureml.TrackFolders|DEBUG|[START]\\n2022-01-24 13:25:43,136|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2022-01-24 13:25:43,136|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2022-01-24 13:25:43,136|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4\\n2022-01-24 13:25:43,136|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2022-01-24 13:25:43,136|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4\\n2022-01-24 13:25:43,146|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2022-01-24 13:25:43,146|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2022-01-24 13:25:43,277|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:43,382|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/14301_azureml.log path: /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4/logs/azureml/14301_azureml.log\\n2022-01-24 13:25:43,382|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2022-01-24 13:25:43,384|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:43,387|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 0_result to queue of approximate size: 0\\n2022-01-24 13:25:44,145|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2022-01-24 13:25:44,145|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2022-01-24 13:25:44,145|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2022-01-24 13:25:44,145|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,146|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,146|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,146|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,146|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,146|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,146|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,172|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2022-01-24 13:25:44,172|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2022-01-24 13:25:44,210|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:44,211|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'b14da3da-8307-42dd-b852-f895db1d5db0', 'azureml.git.repository_uri': 'https://github.com/balakreshnan/AMLBook2022.git', 'mlflow.source.git.repoURL': 'https://github.com/balakreshnan/AMLBook2022.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2', 'mlflow.source.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2', 'azureml.git.dirty': 'False'}\\n2022-01-24 13:25:44,211|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2022-01-24 13:25:44,362|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2022-01-24 13:25:44,362|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2022-01-24 13:25:44,363|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2022-01-24 13:25:44,363|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,363|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,363|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,363|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,363|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,364|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,364|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,369|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient.patch_by_exp_id-async:False|DEBUG|[START]\\n2022-01-24 13:25:44,369|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling patch_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2022-01-24 13:25:44,450|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.RunClient.patch_by_exp_id-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:44,471|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2022-01-24 13:25:44,472|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2022-01-24 13:25:44,472|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2022-01-24 13:25:44,473|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,474|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,480|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,481|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,481|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,482|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:44,482|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://centralus.api.azureml.ms.\\n2022-01-24 13:25:46,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2022-01-24 13:25:46,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.PostMetricsBatchV2Daemon|DEBUG|Starting daemon and triggering first instance\\n2022-01-24 13:25:46,546|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2022-01-24 13:25:46,637|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Called upload_artifact\\n2022-01-24 13:25:46,637|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Uploading io artifact\\n2022-01-24 13:25:46,638|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2022-01-24 13:25:46,638|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2022-01-24 13:25:46,793|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:46,793|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2022-01-24 13:25:46,850|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/ROC_1643030746.png with size 22654, file size 22654.\\n2022-01-24 13:25:46,853|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Called upload_artifact\\n2022-01-24 13:25:46,854|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Uploading io artifact\\n2022-01-24 13:25:46,854|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2022-01-24 13:25:46,854|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2022-01-24 13:25:47,051|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:47,051|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2022-01-24 13:25:47,089|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/ConfusionMatrix_Test with size 130, file size 130.\\n2022-01-24 13:25:47,093|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Called upload_artifact\\n2022-01-24 13:25:47,093|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Uploading path artifact\\n2022-01-24 13:25:47,094|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2022-01-24 13:25:47,094|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2022-01-24 13:25:47,394|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:47,395|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2022-01-24 13:25:47,433|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/titanic_model.pkl with size 5202, file size 5202.\\n2022-01-24 13:25:47,433|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|Fetching files for prefix in ExperimentRun, dcid.titanic_remote_compute_1643030740_5938d4b4, titanic_model.pkl\\n2022-01-24 13:25:47,433|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.list_sas_by_prefix-async:True|DEBUG|[START]\\n2022-01-24 13:25:47,433|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: _execute_with_base_arguments\\n2022-01-24 13:25:47,434|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling list_sas_by_prefix with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/prefix/contentinfo/{origin}/{container}/{path}\\n2022-01-24 13:25:47,434|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.list_sas_by_prefix|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:47,436|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.list_sas_by_prefix-async:True|DEBUG|[STOP]\\n2022-01-24 13:25:47,436|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.list_sas_by_prefix.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:47,436|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.list_sas_by_prefix.WaitingTask|DEBUG|Awaiter is ApiPagination\\n2022-01-24 13:25:47,490|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.list_sas_by_prefix.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:47,490|azureml._restclient.clientbase|DEBUG|Found continuation_token field in DTO\\n2022-01-24 13:25:47,490|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.AssetsClient.create-async:False|DEBUG|[START]\\n2022-01-24 13:25:47,491|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.AssetsClient|DEBUG|ClientBase: Calling create with url /modelmanagement/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}/assets\\n2022-01-24 13:25:47,546|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Start]\\n2022-01-24 13:25:47,546|azureml.BatchTaskQueueAdd_1_Batches.WorkerPool|DEBUG|submitting future: _handle_batch\\n2022-01-24 13:25:47,547|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Batch size 4.\\n2022-01-24 13:25:47,547|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:47,547|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: _log_batch_v2\\n2022-01-24 13:25:47,547|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Adding task 0__handle_batch to queue of approximate size: 0\\n2022-01-24 13:25:47,547|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|[Stop] - waiting default timeout\\n2022-01-24 13:25:47,547|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[START]\\n2022-01-24 13:25:47,548|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Overriding default flush timeout from None to 120\\n2022-01-24 13:25:47,548|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0__handle_batch)].\\n2022-01-24 13:25:47,548|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|Metrics Client: _log_batch_v2 is calling post_run_metrics posting 4 values.\\n2022-01-24 13:25:47,548|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient._post_run_metrics_log_failed_validations-async:False|DEBUG|[START]\\n2022-01-24 13:25:47,548|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|ClientBase: Calling _post_run_metrics_log_failed_validations with url None\\n2022-01-24 13:25:47,552|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:47,552|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|Adding task 0__log_batch_v2 to queue of approximate size: 0\\n2022-01-24 13:25:47,709|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.AssetsClient.create-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:47,715|azureml.ModelsClient.register-async:False|DEBUG|[START]\\n2022-01-24 13:25:47,715|azureml.ModelsClient|DEBUG|ClientBase: Calling register with url /modelmanagement/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}/models\\n2022-01-24 13:25:47,798|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:47,798|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|Awaiter is BatchTaskQueueAdd_1_Batches\\n2022-01-24 13:25:47,798|azureml.BatchTaskQueueAdd_1_Batches.0__handle_batch.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:47,798|azureml.BatchTaskQueueAdd_1_Batches|DEBUG|Waiting on task: 0__handle_batch.\\n1 tasks left. Current duration of flush 8.20159912109375e-05 seconds.\\n\\n2022-01-24 13:25:47,798|azureml.BatchTaskQueueAdd_1_Batches.WaitFlushSource:BatchTaskQueueAdd_1_Batches|DEBUG|[STOP]\\n2022-01-24 13:25:47,814|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient._post_run_metrics_log_failed_validations-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:48,145|azureml.ModelsClient.register-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:48,146|azureml.WorkspaceClient.get-async:False|DEBUG|[START]\\n2022-01-24 13:25:48,146|azureml.WorkspaceClient|DEBUG|ClientBase: Calling get with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experiments/{experimentName}\\n2022-01-24 13:25:48,191|azureml.WorkspaceClient.get-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:48,192|azureml.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2022-01-24 13:25:48,192|azureml.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2022-01-24 13:25:48,239|azureml.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:48,240|azureml.core.run|DEBUG|Available factories for run types {'azureml.scriptrun': <function ScriptRun._from_run_dto at 0x7f8a07030158>}\\n2022-01-24 13:25:48,240|azureml.core.run|DEBUG|Initializing Run titanic_remote_compute_1643030740_5938d4b4 from type azureml.scriptrun\\n2022-01-24 13:25:48,242|azureml.ScriptRun#titanic_remote_compute_1643030740_5938d4b4|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'b14da3da-8307-42dd-b852-f895db1d5db0', 'azureml.git.repository_uri': 'https://github.com/balakreshnan/AMLBook2022.git', 'mlflow.source.git.repoURL': 'https://github.com/balakreshnan/AMLBook2022.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2', 'mlflow.source.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2', 'azureml.git.dirty': 'False'}\\n2022-01-24 13:25:48,242|azureml.ScriptRun#titanic_remote_compute_1643030740_5938d4b4.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2022-01-24 13:25:48,242|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4|INFO|complete is not setting status for submitted runs.\\n2022-01-24 13:25:48,242|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[START]\\n2022-01-24 13:25:48,242|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|Overrides: Max batch size: 50, batch cushion: 5, Interval: 1.\\n2022-01-24 13:25:48,242|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatch.PostMetricsBatchDaemon|DEBUG|Starting daemon and triggering first instance\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|Used <class 'azureml._common.async_utils.batch_task_queue.BatchTaskQueue'> for use_batch=True.\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [].\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatch|DEBUG|\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatch.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[START]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|flush timeout 300 is different from task queue timeout 120, using flush timeout\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|Waiting 300 seconds on tasks: [AsyncTask(0__log_batch_v2)].\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2.WaitingTask|DEBUG|Awaiter is PostMetricsBatchV2\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.0__log_batch_v2.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2|DEBUG|\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.PostMetricsBatchV2.WaitFlushSource:MetricsClient|DEBUG|[STOP]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.FlushingMetricsClient|DEBUG|[STOP]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[START]\\n2022-01-24 13:25:48,243|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient|DEBUG|ClientBase: Calling wait_on_ingest with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experiments/{experimentName}/runs/{runId}/metricsingest/wait\\n2022-01-24 13:25:48,289|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.MetricsClient.wait_on_ingest-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:48,290|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Uploading tracked directories: [], excluding []\\n2022-01-24 13:25:48,290|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling track for pyfs\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Reverting working dir from /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4 to /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory|INFO|Working dir is already updated /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[STOP]\\n2022-01-24 13:25:48,361|azureml.WorkingDirectoryCM|DEBUG|[STOP]\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Uploading tracked directories: ['./outputs'], excluding ['azureml-logs/driver_log']\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling track for pyfs\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory|DEBUG|./outputs exists as directory, uploading..\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Found and adding path to upload: ./outputs/titanic_model.pkl\\n2022-01-24 13:25:48,361|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Paths to upload is ['./outputs/titanic_model.pkl'] in dir ./outputs\\n2022-01-24 13:25:48,362|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Overriding default timeout to 300\\n2022-01-24 13:25:48,362|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Start]\\n2022-01-24 13:25:48,362|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2022-01-24 13:25:48,362|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2022-01-24 13:25:48,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:48,545|azureml._restclient.service_context.WorkerPool|DEBUG|submitting future: perform_upload\\n2022-01-24 13:25:48,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:48,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Adding task 0_perform_upload to queue of approximate size: 0\\n2022-01-24 13:25:48,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|[Stop] - waiting default timeout\\n2022-01-24 13:25:48,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[START]\\n2022-01-24 13:25:48,545|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Overriding default flush timeout from None to 300\\n2022-01-24 13:25:48,546|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|Waiting 300 seconds on tasks: [AsyncTask(0_perform_upload)].\\n2022-01-24 13:25:48,546|azureml._restclient.clientbase|DEBUG|ClientBase: Calling create_blob_from_stream with url None\\n2022-01-24 13:25:48,617|azureml._file_utils.upload|DEBUG|Uploaded blob ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/outputs/titanic_model.pkl with size 5202, file size 5202.\\n2022-01-24 13:25:48,796|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:48,796|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|Awaiter is upload_files\\n2022-01-24 13:25:48,796|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.0_perform_upload.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:48,796|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files|DEBUG|Waiting on task: 0_perform_upload.\\n1 tasks left. Current duration of flush 6.961822509765625e-05 seconds.\\n\\n2022-01-24 13:25:48,796|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.upload_files.WaitFlushSource:upload_files|DEBUG|[STOP]\\n2022-01-24 13:25:48,797|azureml.TrackFolders|DEBUG|[STOP]\\n2022-01-24 13:25:48,797|azureml._history.utils.context_managers|DEBUG|exiting ContentUploader, waiting for file_watcher to finish upload...\\n2022-01-24 13:25:48,797|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher called finish, setting event\\n2022-01-24 13:25:48,797|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher received exit event, getting current_stat\\n2022-01-24 13:25:48,797|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[START]\\n2022-01-24 13:25:48,798|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient|DEBUG|ClientBase: Calling batch_create_empty_artifacts with url /artifact/v2.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/artifacts/batch/metadata/{origin}/{container}\\n2022-01-24 13:25:48,981|azureml._SubmittedRun#titanic_remote_compute_1643030740_5938d4b4.RunHistoryFacade.ArtifactsClient.batch_create_empty_artifacts-async:False|DEBUG|[STOP]\\n2022-01-24 13:25:49,012|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess.log path: /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess.log\\n2022-01-24 13:25:49,052|azureml._history.utils.context_managers.FileWatcher|DEBUG|uploading data to container: azureml blob: ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess_Telemetry.log path: /tmp/azureml_runs/titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess_Telemetry.log\\n2022-01-24 13:25:49,052|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2022-01-24 13:25:49,052|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:49,052|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 1_result to queue of approximate size: 1\\n2022-01-24 13:25:49,053|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2022-01-24 13:25:49,053|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:49,056|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 2_result to queue of approximate size: 2\\n2022-01-24 13:25:49,059|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2022-01-24 13:25:49,060|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:49,060|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 3_result to queue of approximate size: 3\\n2022-01-24 13:25:49,060|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher retrieved current_stat, will upload to current_stat\\n2022-01-24 13:25:49,060|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,063|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,064|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,064|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,064|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,064|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,064|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,065|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,065|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,065|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,066|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WorkerPool|DEBUG|submitting future: result\\n2022-01-24 13:25:49,066|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result|DEBUG|Using basic handler - no exception handling\\n2022-01-24 13:25:49,067|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Adding task 4_result to queue of approximate size: 4\\n2022-01-24 13:25:49,067|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,070|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,071|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,071|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,071|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,071|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher uploading files to current_stat...\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher|DEBUG|FileWatcher finished uploading to current_stat, finishing task queue\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|[Stop] - waiting default timeout\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[START]\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Overriding default flush timeout from None to 120\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|Waiting 120 seconds on tasks: [AsyncTask(0_result), AsyncTask(1_result), AsyncTask(2_result), AsyncTask(3_result), AsyncTask(4_result)].\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.0_result.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:49,072|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.1_result.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.2_result.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2022-01-24 13:25:49,073|azureml._history.utils.context_managers.FileWatcher.UploadQueue.3_result.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:49,323|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[START]\\n2022-01-24 13:25:49,323|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|Awaiter is UploadQueue\\n2022-01-24 13:25:49,323|azureml._history.utils.context_managers.FileWatcher.UploadQueue.4_result.WaitingTask|DEBUG|[STOP]\\n2022-01-24 13:25:49,324|azureml._history.utils.context_managers.FileWatcher.UploadQueue|DEBUG|Waiting on task: 4_result.\\n1 tasks left. Current duration of flush 0.00049591064453125 seconds.\\n\\n2022-01-24 13:25:49,324|azureml._history.utils.context_managers.FileWatcher.UploadQueue.WaitFlushSource:UploadQueue|DEBUG|[STOP]\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.36.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SynapseCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'titanic_remote_compute_1643030740_5938d4b4',\n",
       " 'target': 'local',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2022-01-24T13:25:41.170199Z',\n",
       " 'services': {},\n",
       " 'properties': {'_azureml.ComputeTargetType': 'local',\n",
       "  'ContentSnapshotId': 'b14da3da-8307-42dd-b852-f895db1d5db0',\n",
       "  'azureml.git.repository_uri': 'https://github.com/balakreshnan/AMLBook2022.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/balakreshnan/AMLBook2022.git',\n",
       "  'azureml.git.branch': 'main',\n",
       "  'mlflow.source.git.branch': 'main',\n",
       "  'azureml.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2',\n",
       "  'mlflow.source.git.commit': 'fb0eb505a9199147bba52cf3ee1571aad910ebd2',\n",
       "  'azureml.git.dirty': 'False'},\n",
       " 'inputDatasets': [{'dataset': {'id': 'cb7282d6-14b7-41f8-b452-4c0343e764c2'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'titanic', 'mechanism': 'Direct'}}, {'dataset': {'id': 'cb7282d6-14b7-41f8-b452-4c0343e764c2'}, 'consumptionDetails': {'type': 'Reference'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'training.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--input-data', 'DatasetConsumptionConfig:titanic'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'local',\n",
       "  'dataReferences': {},\n",
       "  'data': {'titanic': {'dataLocation': {'dataset': {'id': 'cb7282d6-14b7-41f8-b452-4c0343e764c2',\n",
       "      'name': 'Titanic-tabular-dataset',\n",
       "      'version': '3'},\n",
       "     'dataPath': None,\n",
       "     'uri': None},\n",
       "    'mechanism': 'Direct',\n",
       "    'environmentVariableName': 'titanic',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False,\n",
       "    'options': None}},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 1,\n",
       "  'instanceTypes': [],\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'experiment-env',\n",
       "   'version': 'Autosave_2022-01-24T12:59:14Z_64bf7164',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'dependencies': ['python=3.6.2',\n",
       "      'scikit-learn',\n",
       "      'ipykernel',\n",
       "      'matplotlib',\n",
       "      'pandas',\n",
       "      'pip',\n",
       "      {'pip': ['azureml-defaults', 'pyarrow']}],\n",
       "     'name': 'azureml_0c5a9aa2def4b3c2501c1f40287a356b'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': 'D2',\n",
       "   'imageVersion': 'pytorch-1.7.0',\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'sshPublicKeys': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': 'Medium',\n",
       "   'slaTier': 'Standard',\n",
       "   'userAlias': None},\n",
       "  'kubernetesCompute': {'instanceType': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': []},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/azureml-logs/60_control_log.txt?sv=2019-07-07&sr=b&sig=KDcUPQkkPFUgOZZiXPlxiHGJcg7BYW0KmxpOA9t6Eqg%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=jMhAYhMQjd77jxOLBq5GSAtcPIkBHhBlTvSHE25YReM%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r',\n",
       "  'logs/azureml/14301_azureml.log': 'https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/14301_azureml.log?sv=2019-07-07&sr=b&sig=l59MURGsFgD5%2BJKcWtYCmscK7ERxLNT%2F2lIaiLqBWpg%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess.log': 'https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=%2BhLc11GyLmXD87P8k8RH8b1QJFO%2BELv9bl%2Bxvp0YBTw%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://mlopsdev3695286978.blob.core.windows.net/azureml/ExperimentRun/dcid.titanic_remote_compute_1643030740_5938d4b4/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=vaGUKaGb8rjPaGO%2FF5i%2Fvy%2FgpgnFZ9reS8ksC3y%2BQ2c%3D&skoid=79188af8-05f3-4945-aaf5-b6abcecc3006&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-24T12%3A45%3A57Z&ske=2022-01-25T20%3A55%3A57Z&sks=b&skv=2019-07-07&st=2022-01-24T13%3A15%3A49Z&se=2022-01-24T21%3A25%3A49Z&sp=r'},\n",
       " 'submittedBy': 'Balamurugan Balakreshnan'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import azureml.core.runconfig\n",
    "from azureml.core import Environment, Experiment\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Get the training dataset\n",
    "titanic_ds = ws.datasets.get('Titanic-tabular-dataset')\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=script_folder,\n",
    "                                script='training.py',\n",
    "                                arguments=['--input-data', titanic_ds.as_named_input('titanic')], # Reference to dataset\n",
    "                                environment=experiment_env) \n",
    "\n",
    "# submit the experiment\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0704db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
